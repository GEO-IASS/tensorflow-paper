\section{Comparison With Other Deep Learning Frameworks}\label{sec:comp}

Next to TensorFlow, there exist a number of other open source deep learning
software libraries, the most popular being Theano, Torch and Caffe. In this
section, we review two sources of quantitative comparisons between TensorFlow
and these alternatives, providing a summary of the most important results of
each work.

The first source in our collection is the \emph{convnet-benchmarks} repository
on GitHub by Soumith Chintala \cite{convnet-bench}, a research engineer at
Facebook. The commit we reference\footnote{Commit hash:
  84b5bb1785106e89691bc0625674b566a6c02147} is dated April 25, 2016. Chintala
provides an extensive set of benchmarks for a variety of convolutional network
models and many libraries. Inter alia, Chintala gives the forward and
backward-propagation time of TensorFlow, Torch and Caffe for the AlexNet CNN
model \cite{alexnet}. Theano is not included in these benchmarks. As shown in
Table \ref{tab:convnet}, TensorFlow performs second-best in both measures behind
Torch, with Caffe lagging relatively far behind.

\begin{table}
  \centering
  \begin{tabular}{ccc}
    \textbf{Library} & \textbf{Forward (ms)} & \textbf{Backward (ms)}
    \\ \toprule
    TensorFlow & 26  & 55
    \\
    Torch & \textbf{25} & \textbf{46}
    \\
    Caffe & 121 & 203
    \\ \bottomrule
  \end{tabular}
  \caption{The result of Soumith Chintala's benchmarks for TensorFlow, Torch and
    Caffe (not Theano) on an AlexNet ConvNet model \cite{alexnet,
      convnet-bench}.}
  \label{tab:convnet}
\end{table}

Further benchmarks were published by the Theano development team on May 9, 2016
\cite{theano}. We focus on their results for an LSTM network operating on the
Penn Treebank dataset \cite{penntreebank}. Their comparisons measure words
processed per second for a small model consisting of a single 200-unit hidden
layer and a large model with two 650-unit hidden layers. Tested libraries
include Theano, Torch and TensorFlow, but not Caffe. In their benchmarks,
TensorFlow performs best among all three for the small model, followed by Theano
and then Torch. For the large model, TensorFlow is placed second behind Theano,
while Torch remains in last place. Figure \ref{fig:theano-results} shows these
outcomes.

\begin{figure}
  \centering
  \begin{tikzpicture}
    % Frame
    \draw (0, 0) rectangle (6, 4);

    % Ticks + Scale
    \foreach \i in {2, 4, ..., 18} {
      % Left
      \draw (0, {\i / 5}) -- ++(+0.15, 0);
      % Right
      \draw (5.85, {\i / 5}) -- ++(+0.15, 0);
      % Labels
      \draw (-0.3, {\i / 5}) node {\i};
    }

    % Legend
    \draw (-0.9, 2.1) node [rotate=90] {$10^3$ words/sec};

    \fill [SkyBlue] (3.4, 3.5) rectangle ++(0.3, 0.3);
    \draw (4.4, 3.65) node {Theano};

    \fill [teal] (3.4, 3) rectangle ++(0.3, 0.3);
    \draw (4.275, 3.15) node {Torch};

    \fill [orange] (3.4, 2.5) rectangle ++(0.3, 0.3);
    \draw (4.7, 2.65) node {TensorFlow};


    %%% Small Model %%%

    % Theano
    \fill [SkyBlue] (0.75, 0) rectangle ++(0.6, 2.75);
    % Torch
    \fill [teal] (1.35, 0) rectangle ++(0.6, 2.4);
    % TensorFlow
    \fill [orange] (1.95, 0) rectangle ++(0.6, 3.4);

    % Label
    \draw (1.8, -0.3) node {Small};

    %%% Large Model %%%

    % Theano
    \fill [SkyBlue] (3.45, 0) rectangle ++(0.6, 2);
    % Torch
    \fill [teal] (4.05, 0) rectangle ++(0.6, 1.1);
    % TensorFlow
    \fill [orange] (4.65, 0) rectangle ++(0.6, 1.6);

    % Label
    \draw (4.4, -0.3) node {Large};

  \end{tikzpicture}
  \caption{The results of \cite{theano}, comparing TensorFlow, Theano and Torch
    on an LSTM model for the Penn Treebank dataset \cite{penntreebank}. On the
    left the authors tested a small model with a single hidden layer and 200
    units; on the right they use two layers with 650 units each. }
  \label{fig:theano-results}
\end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End: