\section{Introduction}

Modern artificial intelligence systems and machine learning algorithms have
revolutionized approaches to scientific and technological challenges in a
variety of fields. We can observe remarkable improvements in the quality of
state-of-the-art computer vision, natural language processing, speech
recognition and other techniques. Moreover, the benefits of recent breakthroughs
have trickled down to the individual, improving everyday life in numerous
ways. Personalized digital assistants, recommendations on e-commerce platforms,
financial fraud detection, customized web search results and social network
feeds as well as novel discoveries in genomics have all been improved, if not
enabled, by current machine learning methods.

A particular branch of machine learning, \emph{deep learning}, has proven
especially effective in recent years. Deep learning is a family of
representation learning algorithms employing complex neural network
architectures with a high number of hidden layers, each composed of simple but
non-linear transformations to the input data. Given enough such transformation
modules, very complex functions may be modeled to solve classification,
regression, transcription and numerous other learning tasks \cite{nature2015}.

It is noteworthy that the rise in popularity of deep learning can be traced back
to only the last few years, enabled primarily by the greater availability of
large data sets, containing more training examples; the efficient use of
graphical processing units (GPUs) and massively parallel commodity hardware to
train deep learning models on these equally massive data sets as well as the
discovery of new methods such as the \emph{rectified linear unit} (ReLU)
activation function or \emph{dropout} as a regularization technique\cite{relu,
  dropout, nature2015, rampasek}.

While deep learning algorithms and individual architectural components such as
representation transformations, activation functions or regularization methods
may initially be expressed in mathematical notation, they must eventually be
transcribed into a computer program for real world usage. For this purpose,
there exist a number of open source as well as commercial machine learning
software libraries and frameworks. Among these are Theano \cite{theano}, Torch
\cite{torch}, scikit-learn \cite{scikit} and many more, which we review in
further detail in Section II of this paper. In November 2015, this list was
extended by \emph{TensorFlow}, a novel machine learning software library
released by Google \cite{tensorflow}. As per the initial publication, TensorFlow
aims to be ``an interface for expressing machine learning algorithms'' in
``large-scale [\dots] on heterogeneous distributed systems'' \cite{tensorflow}.

The remainder of this paper aims to give a thorough review of TensorFlow and put
it in context of the current state of machine learning. In detail, the paper is
further structured as follows. Section \ref{sec:history} will provide a brief
overview and history of machine learning software libraries, listing but not
comparing projects similar to TensorFlow. Subsequently, Section \ref{sec:model}
discusses in depth the computational paradigms underlying TensorFlow. In Section
\ref{sec:code} we explain the current programming interface in the various
supported languages. To inspect and debug models expressed in TensorFlow, there
exist powerful visualization tools, which we examine in Section
\ref{sec:visual}. Section \ref{sec:comp} then gives a comparison of TensorFlow
and alternative deep learning libraries on a qualitative as well as quantitative
basis. Before concluding our review in Section \ref{sec:conclusion}, Section
\ref{sec:uses} studies current real world use cases of TensorFlow in literature
and industry.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End:
