\section{The TensorFlow Programming Interface}\label{sec:code}

Having conveyed the abstract concepts of TensorFlow's computational model in
Section \ref{sec:model}, we will now concretize those ideas and speak to
TensorFlow's programming interface. We begin with a brief discussion of the
available language interfaces. Then, we provide a more hands-on look at
TensorFlow's Python API by walking through a simple practical example.

\subsection{Interfaces}\label{sec:code-interfaces}

There currently exist two programming interfaces, in C++ and Python, that permit
interaction with the TensorFlow backend. The Python API boasts a very rich
feature set for creation and execution of computational graphs. As of this
writing, the C++ interface (which is really just the core backend
implementation) provides a comparatively much more limited API, allowing only to
execute graphs built with Python and serialized to Google's Protocol
Buffer\footnote{https://developers.google.com/protocol-buffers/} format. While
there is experimental support for also building computational graphs in C++,
this functionality is currently not as extensive as in Python.

It is noteworthy that the Python API integrates very well with
NumPy\footnote{http://www.numpy.org}, a popular open source Python numeric and
scientific programming library. As such, TensorFlow tensors may be interchanged
with NumPy \texttt{ndarrays} in many places.

\subsection{Walkthrough}\label{sec:code-walk}

In the following paragraphs we give a step-by-step walkthrough of a practical,
real-world example of TensorFlow's Python API. We will train a simple
multi-layer perceptron (MLP) with one input and one output layer to classify
handwritten digits in the MNIST\footnote{http://yann.lecun.com/exdb/mnist/}
dataset. In this dataset, the examples are small images of $28 \times 28$ pixels
depicting handwritten digits in $\in \{0, \dots, 9\}$. We receive each such
example as a flattened vector of $784$ gray-scale pixel intensities. The label
for each example is the digit it is supposed to represent.

We begin our walkthrough by importing the TensorFlow library and reading the
MNIST dataset into memory. For this we assume a utility module
\texttt{mnist\_data} with a method \texttt{read} which expects a path to extract
and store the dataset. Moreover, we pass the parameter \texttt{one\_hot=True} to
specify that each label be given to us as a \emph{one-hot-encoded} vector
$(d_1, \dots, d_{10})^\top$ where all but the $i$-th component are set to zero
if an example represents the digit $i$:

\begin{lstlisting}
import tensorflow as tf

# Download and extract the MNIST data set.
# Retrieve the labels as one-hot-encoded vectors.
mnist = mnist_data.read("/tmp/mnist", one_hot=True)
\end{lstlisting}

Next, we create a new computational graph via the \texttt{tf.Graph}
constructor. To add operations to this graph, we must register it as the
\emph{default graph}. The way the TensorFlow API is designed, library routines
that create new operation nodes always attach these to the current default
graph. We register our graph as the default by using it as a Python context
manager in a \texttt{with-as} statement:

\begin{lstlisting}
# Create a new graph
graph = tf.Graph()

# Register the graph as the default one to add nodes
with graph.as_default():
  # Add operations ...
\end{lstlisting}

We are now ready to populate our computational graph with operations. We begin
by adding two \emph{placeholder} nodes \texttt{examples} and
\texttt{labels}. Placeholders are special variables that \emph{must} be replaced
with concrete tensors upon graph execution. That is, they must be supplied in
the \texttt{feed\_dict} argument to \texttt{Session.run()}, mapping tensors to
replacement values. For each such placeholder, we specify a shape and
data type. An interesting feature of TensorFlow at this point is that we may
specify the Python keyword \texttt{None} for the first dimension of each
placeholder shape. This allows us to later on feed a tensor of variable size in
that dimension. For the column size of the \texttt{example} placeholder, we
specify the number of features for each image, meaning the $28 \times 28 = 784$
pixels. The label placeholder should expect 10 columns, corresponding to the
10-dimensional one-hot-encoded vector for each label digit:

\begin{lstlisting}
# Using a 32-bit floating-point data type tf.float32
examples = tf.placeholder(tf.float32, [None, 784])
labels = tf.placeholder(tf.float32, [None, 10])
\end{lstlisting}

Given an example matrix $\mathbf{X} \in \mathbb{R}^{n \times 784}$ containing
$n$ images, the learning task then applies an affine transformation
$\mathbf{X} \cdot \mathbf{W} + \mathbf{b}$, where $\mathbf{W}$ is a \emph{weight}
  matrix $\in \mathbb{R}^{784 \times 10}$ and $\mathbf{b}$ a \emph{bias} vector
$\in \mathbb{R}^{10}$. This yields a new matrix
$\mathbf{Y} \in \mathbb{R}^{n \times 10}$, containing the \emph{scores} or
\emph{logits} of our model for each example and each possible digit. These
scores are more or less arbitrary values and not a probability distribution,
i.e. they need neither be $\in [0, 1]$ nor sum to one. To transform the logits
into a valid probability distribution, giving the likelihood $\Pr[x = i]$ that
the $x$-th example represents the digit $i$, we make use of the softmax
function, given in Equation \ref{eq:softmax}. Our final estimates are thus
calculated by $\softmax(\mathbf{X} \cdot \mathbf{W} + \mathbf{b})$, as shown
below:

\begin{lstlisting}
# Draw random weights for symmetry breaking
weights = tf.Variable(tf.random_uniform([784, 10]))
# Slightly positive initial bias
bias = tf.Variable(tf.constant(0.1, shape=[10]))
# tf.matmul performs the matrix multiplication XW
# Note how the + operator is overloaded for tensors
logits = tf.matmul(examples, weights) + bias
# Applies the operation element-wise on tensors
estimates = tf.nn.softmax(logits)
\end{lstlisting}

\begin{equation}\label{eq:softmax}
  \softmax(\mathbf{x})_i = \frac{\exp(\mathbf{x}_i)}{\sum_j \exp(\mathbf{x}_j)}
\end{equation}

Next, we compute our objective function, producing the error or \emph{loss} of
the model given its current trainable parameters $\mathbf{W}$ and
$\mathbf{b}$. We do this by calculating the \emph{cross entropy}
$H(\mathbf{L}, \mathbf{Y})_i = -\sum_j \mathbf{L}_{i,j} \cdot
\log(\mathbf{Y}_{i,j})$ between the probability distributions of our estimates
$\mathbf{Y}$ and the one-hot-encoded labels $\mathbf{L}$. More precisely, we
consider the mean cross entropy over all examples as the loss:

\begin{lstlisting}
# Computes the cross-entropy and sums the rows
cross_entropy = -tf.reduce_sum(
    labels * tf.log(estimates), [1])
loss = tf.reduce_mean(cross_entropy)
\end{lstlisting}

Now that we have an objective function, we can run (stochastic) gradient descent
to update the weights of our model. For this, TensorFlow provides a
\texttt{GradientDescentOptimizer} class. It is initialized with the
learning rate of the algorithm and provides an operation \texttt{minimize}, to
which we pass our \texttt{loss} tensor. This is the operation we will run
repeatedly in a \texttt{Session} environment to train our model:

\begin{lstlisting}
# We choose a learning rate of 0.5
gdo = tf.train.GradientDescentOptimizer(0.5)
optimizer = gdo.minimize(loss)
\end{lstlisting}

Finally, we can actually train our algorithm. For this, we enter a session
environment using a \texttt{tf.Session} as a context manager. We pass our graph
object to its constructor, so that it knows which graph to manage. To then
execute nodes, we have several options. The most general way is to call
\texttt{Session.run()} and pass a list of tensors we wish to
compute. Alternatively, we may call \texttt{eval()} on tensors and
\texttt{run()} on operations directly. Before evaluating any other node, we must
first ensure that the variables in our graph are initialized. Theoretically, we
could \texttt{run} the \texttt{Variable.initializer} operation for each
variable. However, one most often just uses the
\texttt{tf.initialize\_all\_variables()} utility operation provided by
TensorFlow, which in turn executes the \texttt{initializer} operation for each
\texttt{Variable} in the graph. Then, we can perform a certain number of
iterations of stochastic gradient descent, fetching an example and label
mini-batch from the MNIST dataset each time and feeding it to the \texttt{run}
function of our optimizer. At the end, our loss will (hopefully) be small:

\begin{lstlisting}
with tf.Session(graph=graph) as session:
    # Execute the operation directly
    tf.initialize_all_variables().run()
    for step in range(1000):
        # Fetch next 100 examples and labels
        x, y = mnist.train.next_batch(100)
        # Ignore the result of the optimizer (None)
        _, loss_value = session.run(
            [optimizer, loss],
            feed_dict={examples: x, labels: y})
        print('Loss at step {0}: {1}'
                .format(step, loss_value))
\end{lstlisting}

The full code listing for this example, along with some additional
implementation to compute an accuracy metric at each time step is given in
Appendix \ref{app:code}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End: