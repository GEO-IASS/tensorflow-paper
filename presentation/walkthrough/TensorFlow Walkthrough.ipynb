{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import the training data (MNIST)\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the tensorflowing begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly download and extract the MNIST data set.\n",
    "Retrieve the labels as one-hot-encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new `Graph` and register it as the *default graph* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create placeholders for examples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Placeholder for input examples (None = variable dimension)\n",
    "    examples = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    # Placeholder for labels\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Weight and bias variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Draw the weights from a random uniform distribution for symmetry breaking\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[784, 10], stddev=0.1))\n",
    "    # Slightly positive initial bias to avoid dead neurons\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apply the affine transformation to the input features to produce *scores*\n",
    "2. Run the softmax function over the scores to create a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # First get the logits\n",
    "    logits = tf.matmul(examples, weights) + bias\n",
    "    estimates = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our estimates, we want to compute some cost metric that tells us how accurate our model is. \n",
    "For this, we use the cross entropy between the softmax probabilities and the label \"distribution\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Compute the cross-entropy\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(estimates),\n",
    "                                   reduction_indices=[1])\n",
    "    # And finally the loss\n",
    "    loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient descent to minimize the loss.\n",
    "There exist also other optimizers, such as the `MomentumOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Create a gradient-descent optimizer that minimizes the loss.\n",
    "    # We choose a learning rate of 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy to let us know how well our model is doing (it is more descriptive than the loss alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # See if the likeliest prediction matches the label for each row\n",
    "    correct_predictions = tf.equal(\n",
    "        tf.argmax(estimates, dimension=1),\n",
    "        tf.argmax(labels, dimension=1))\n",
    "    # correct predictions / all predictions\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually run and train our algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter a session environment\n",
    "# Pass our graph as the graph to be managed\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Must first initialize variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(1001):\n",
    "    # Grab next example and label batches\n",
    "    example_batch, label_batch = mnist.train.next_batch(100)\n",
    "    # Replace the placeholder tensors with\n",
    "    feed_dict = {examples: example_batch, labels: label_batch}\n",
    "    # Compute the accuracy every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        # Ignore the Optimizer's None return value\n",
    "        _, loss_value, accuracy_value = session.run(\n",
    "           [optimizer, loss, accuracy],\n",
    "           feed_dict=feed_dict\n",
    "        )\n",
    "        print(\"Loss at time {0}: {1}\".format(step, loss_value))\n",
    "        print(\"Accuracy at time {0}: {1}\".format(step, accuracy_value))\n",
    "    else:\n",
    "        # Run the optimizer directly!\n",
    "        optimizer.run(feed_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
